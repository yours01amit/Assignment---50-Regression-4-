{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso Regression\n",
    "    - It stands for Least Absolute Shrinkage and Selection Operator regularization.\n",
    "    - It reduces model complexity and prevents overfitting by adding a penalty term to the cost function.\n",
    "    - The penalty term is proportional to the absolute value of the coefficients of the model.\n",
    "    - This penalty term forces some of the coefficients to become zero, effectively performing feature selection.\n",
    "    - It is more appropriate when there are many features but only a few are expected to be relevant.\n",
    "    #\n",
    "- It is different from other regression techniques:\n",
    "    - Feature Selection\n",
    "        - Lasso regression can perform feature selection by setting some of the coefficients to exactly zero.\n",
    "    - Bias-Variance Trade-Off\n",
    "        - By setting some of the coefficients to zero, Lasso regression can reduce the variance of the model, but this may come at the cost of increased bias.\n",
    "    - Non Linear Relationship\n",
    "        - It can capture nonlinear relationships by including polynomial or interaction terms in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage for lasso regression is that it shrinks the coefficients towards zero and can also force some of them to exactly zero, thus helping in removing irrelevant features from the model and in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the coefficients of a Lasso Regression model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The interpretions of the coefficients of a Lasso Regression model are:\n",
    "    - When analyzing the relationship between predictor variables and the response variable, each coefficient represents how much the response variable changes with a one-unit increase in the corresponding predictor variable.\n",
    "    - It's important to note that some coefficients may be zero, indicating that certain predictor variables aren't contributing to the model and can be excluded.\n",
    "    - The strength of the regularization parameter affects the magnitude of these coefficients, so it's crucial to carefully choose this parameter to balance bias-variance trade-off.\n",
    "    - By examining the sign of these coefficients, we can gain insights into whether there is a positive or negative relationship between predictor variables and response variable; a positive coefficient indicates a positive relationship while a negative one indicates a negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are the main tuning parameters and their effects in Lasso Regression\n",
    "    - Regularization parameter (alpha)\n",
    "        - It controls the strength of the L1 penalty, which is added to the objective function to avoid overfitting and improve the generalization performance of the model.\n",
    "    - Maximum number of iterations\n",
    "        - As it uses an iterative algorithm to minimize the objective function and find the optimal coefficients, The maximum number of iterations determines the maximum number of times the algorithm will run before stopping, even if convergence has not been reached.\n",
    "    - Convergence tolerance\n",
    "        - The convergence tolerance specifies the threshold at which the algorithm will stop iterating and consider that convergence has been reached.\n",
    "    - Standardization\n",
    "        - Lasso Regression assumes that the predictor variables are standardized, meaning that they have a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso regression is a linear regression technique and assumes a linear relationship between the features and the response variable.\n",
    "- it can capture nonlinear relationships by including polynomial or interaction terms in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso regularization\n",
    "    - It stands for Least Absolute Shrinkage and Selection Operator regularization.\n",
    "    - It reduces model complexity and prevents overfitting by adding a penalty term to the cost function.\n",
    "    - The penalty term is proportional to the absolute value of the coefficients of the model.\n",
    "    - This penalty term forces some of the coefficients to become zero, effectively performing feature selection.\n",
    "    - It is more appropriate when there are many features but only a few are expected to be relevant.\n",
    "#\n",
    "- Ridge regularization\n",
    "    - It adds a penalty term proportional to the square of the coefficients.\n",
    "    - It shrinks coefficients towards zero but never forces them to be exactly zero.\n",
    "    - Ridge regularization is more appropriate when all features are expected to be relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso Regression is capable of handling multicollinearity in input features to a certain extent. Multicollinearity arises when two or more predictor variables are closely correlated, making it challenging to distinguish their individual effects on the response variable.\n",
    "- Lasso Regression can tackle this issue by choosing only a subset of input features and eliminating redundant variables that exhibit high correlation with each other, thereby mitigating the impact of multicollinearity.\n",
    "- It's important to note that the selection of important variables may vary depending on the values of the regularization parameter and correlation structure of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When building a model with Lasso Regression, selecting the ideal value for the regularization parameter (lambda) is crucial to ensure accuracy and reliability.\n",
    "- There are various methods to determine the optimal value of lambda, including cross-validation, information criteria, and grid search.\n",
    "#\n",
    " - One approach is to use k-fold cross-validation\n",
    "    - The dataset is split into k folds, and the model is trained and evaluated k times using a different subset each time for validation. The average performance across all iterations can help identify the best lambda value that minimizes validation error.\n",
    "#\n",
    "- Infromation Critria\n",
    "    - This method involves using information criteria such as Akaike information criterion (AIC) or Bayesian information criterion (BIC)\n",
    "    - These criteria balance the goodness of fit of the model with its complexity by penalizing models with too many parameters. The ideal lambda value would be one that minimizes this criterion.\n",
    "#\n",
    "- Grid Search   \n",
    "    - A grid search can be used to evaluate performance metrics such as accuracy or R-squared for various values of lambda in a given range. The optimal value would be one that maximizes these metrics.\n",
    "#\n",
    "- It's important to note that selecting an approach depends on specific data characteristics and problem at hand.\n",
    "- It's also recommended to validate any selected lambda values on independent test sets for generalization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
